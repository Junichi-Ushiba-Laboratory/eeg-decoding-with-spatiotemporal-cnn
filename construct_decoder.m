

inputsize = config_dataset.get_inputsize;

num_conv_temp_long  = config_dataset.time_seg*config_dataset.fs;
num_conv_temp_short = config_dataset.fs*0.2;
N = 15;
M = 15;
poolsize = 20;
num_stride = 6;
num_pooled = ceil(num_conv_temp_long/num_stride);
    
maxEpochs = 500;
miniBatchSize = 32;

options = trainingOptions('rmsprop', ...
'InitialLearnRate',0.001,...
'ExecutionEnvironment','parallel', ...
'GradientThreshold',1, ...
'MaxEpochs',maxEpochs, ...
'MiniBatchSize',miniBatchSize, ...
'SequenceLength','longest', ...
'Shuffle','every-epoch', ...
'Verbose',1, ...
'VerboseFrequency',60, ...
'ValidationData',{tbl_valid,y_valid'},...
'ValidationPatience',5,...
'ValidationFrequency',100,...
'L2Regularization',1,...
'ResetInputNormalization' ,0 ...
);


layers = [
imageInputLayer(inputsize,"Name","imageinput","Normalization","zerocenter")
dropoutLayer(0.2,"Name","dropout1")
convolution2dLayer([1, num_conv_temp_short],N,"Name","conv_1","Padding","same",...
'WeightL2Factor',0)
batchNormalizationLayer("Name","batchnorm_1",'ScaleL2Factor',0.00000)
groupedConvolution2dLayer([config_dataset.num_ch,1],M,'channel-wise',"Name","conv_3",...
'WeightL2Factor',0.0001)
batchNormalizationLayer("Name","batchnorm_2",'ScaleL2Factor',0.00000)
maxPooling2dLayer([1, poolsize],"Name","maxpool","Padding","same",'Stride',[1,num_stride] ) %Hil
batchNormalizationLayer("Name","batchnorm_3",'ScaleL2Factor',0.00000)
groupedConvolution2dLayer([1,num_pooled],M,'channel-wise',"Name","conv_3",...
'WeightL2Factor',0.0000)
batchNormalizationLayer("Name","batchnorm_4",'ScaleL2Factor',0.00000)
fullyConnectedLayer(1,"Name","fc",...
'WeightL2Factor',0.0000)
batchNormalizationLayer("Name","batchnorm_5")
regressionLayer("Name","regressionoutput")];